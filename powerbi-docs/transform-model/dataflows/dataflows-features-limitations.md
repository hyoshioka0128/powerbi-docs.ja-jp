---
title: データフローの制限事項、制約、サポートされているコネクタと機能
description: データフローのすべての機能の概要
author: davidiseminger
ms.reviewer: ''
ms.service: powerbi
ms.subservice: powerbi-service
ms.topic: how-to
ms.date: 10/01/2020
ms.author: davidi
LocalizationGroup: Data from files
ms.openlocfilehash: 89de77e65d8eb675d9e80c3b2497f39af7c32d33
ms.sourcegitcommit: 37bd34053557089c4fbf0e05f78e959609966561
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 11/10/2020
ms.locfileid: "94396589"
---
# <a name="dataflows-limitations-and-considerations"></a>データフローの制限事項と考慮事項

以下のセクションで説明するように、作成、更新、容量管理に関して、ユーザーが留意する必要のあるデータフローの制限がいくつかあります。

## <a name="dataflow-authoring"></a>データフローの作成

データフローを作成するとき、ユーザーは次の点に注意する必要があります。

* データフローでの作成は、Power Query Online (PQO) 環境で行われます。[Power Query の制限事項](/power-query/power-query-online-limits)に関する記事で説明されている制限事項を参照してください。
データフローの作成は Power Query Online (PQO) 環境で行われるため、データフロー ワークロード構成に対して行われた更新は、更新にのみ反映され、作成エクスペリエンスには反映されません

* データフローは所有者のみが変更できます

* データフローは、" *マイ ワークスペース* " においては利用できません

* ゲートウェイ データ ソースを使用するデータフローにおいては、同じデータ ソースに対する複数の資格情報はサポートされません

* Web.Page コネクタを使用するには、ゲートウェイが必要です

## <a name="api-considerations"></a>API の考慮事項

サポートされている Dataflows REST API の詳細については、[REST API のリファレンス](/rest/api/power-bi/dataflows)を参照してください。 注意すべきいくつかの考慮事項を次に示します。

* データフローをエクスポートおよびインポートすると、そのデータフローに新しい ID が割り当てられます

* リンクされたエンティティが含まれるデータ フローをインポートしても、データフロー内の既存の参照は修正されません (これらのクエリは、データフローをインポートする前に手動で修正する必要があります)

* もともとインポート API を使用して作成されたデータフローは、 *CreateOrOverwrite* パラメーターを使用して上書きできます

## <a name="dataflows-in-shared"></a>共有内のデータフロー

共有容量内のデータフローには制限があります。

* データフローを更新するときの共有でのタイムアウトは、エンティティごとに 2 時間、データフローごとに 3 時間です
* リンクされたエンティティは、共有データフロー内に作成することはできませんが、クエリで " *読み込み有効* " プロパティが無効になっている限り、データフロー内に存在することができます
* 計算対象エンティティを共有データフロー内に作成することはできません
* AutoML と Cognitive Services は、共有データフローでは使用できません
* 増分更新は、共有データフローでは機能しません

## <a name="dataflows-in-premium"></a>Premium でのデータフロー

Premium に存在するデータフローには、次の制限事項と考慮事項があります。

**更新とデータに関する考慮事項:**

* データフローを更新するときのタイムアウトは 24 時間です (エンティティとデータフローの区別はありません)

* 増分更新ポリシーから通常の更新に、またはその逆にデータフローを変更すると、すべてのデータが削除されます

* データフローのスキーマを変更すると、すべてのデータが削除されます

**リンクされたエンティティと計算対象エンティティ:**

* リンクされたエンティティは、32 参照の深さにまですることができます

* リンクされたエンティティの循環依存関係は許可されていません

* オンプレミスのデータ ソースからデータを取得する、通常のエンティティとリンクされたエンティティを結合することはできません

* データフローで別のクエリ (クエリ *B* ) の計算に (たとえばクエリ *A* などの) クエリが使用されている場合、クエリ *B* が計算エンティティになります。 計算エンティティは、オンプレミスのソースを参照できません。


**コンピューティング エンジン:**

* コンピューティング エンジンを使用している間、データ インジェストの時間が最初に約 10% から 20% 増加します。

  1. これは、コンピューティング エンジン上の最初のデータフローでの、データ ソースからのデータの読み取りにのみ該当します
  2. ソース 1 を使用する後続のデータフローにおいては、同じペナルティは発生しません

* 特定の操作によってのみ、リンク エンティティを通して、または計算対象エンティティとして使用される場合にのみ、コンピューティング エンジンが使用されます。 操作の完全な一覧については、[このブログ記事](http://petcu40.blogspot.com/2019/06/m-folding-in-enhanced-engine-of-power.html)を参照してください。


**容量管理:**

* 設計上、Premium Power BI 容量には内部リソース マネージャーがあり、容量が低メモリで実行されているときにさまざまな方法でワークロードが調整されます。

  1. データフローの場合、この調整によって、使用可能な M コンテナーの数が減少します
  2. データフローのメモリは、データ サイズに合わせて適切にサイズ設定されたコンテナーを使用して 100% に設定でき、コンテナーの数はワークロードによって適切に管理されます

* コンテナーのおおよその数は、ワークロードに割り当てられたメモリの総量を、コンテナーに割り当てられたメモリの量で割ることによって確認できます

## <a name="dataflow-usage-in-datasets"></a>データセットでのデータフローの使用

* Power BI Desktop でデータセットを作成した後、それを Power BI サービスに発行する場合、データフローのデータ ソースに対して Power BI Desktop で使用されている資格情報が、データセットがサービスに発行されるときに使用される資格情報と同じであることを確認します。
  1. それらの資格情報が同じでないと、データセットの更新時に " *キーが見つからない* " というエラーが発生します

## <a name="next-steps"></a>次のステップ
データフローと Power BI の詳細については、以下の記事を参照してください。

* [データフローとセルフサービスのデータ準備の概要](dataflows-introduction-self-service.md)
* [データフローの作成](dataflows-create.md)
* [データフローの構成と使用](dataflows-configure-consume.md)
* [Azure Data Lake Gen 2 を使用するようにデータフロー ストレージを構成する](dataflows-azure-data-lake-storage-integration.md)
* [データフローの Premium 機能](dataflows-premium-features.md)
* [データフローを使用した AI](dataflows-machine-learning-integration.md)